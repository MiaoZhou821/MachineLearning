#!/usr/bin/env python3
import numpy as np
import matplotlib.pyplot as plt



DATA_PATH = "/data/" 


def parse_data(args):
    num = float
    dtype = np.float32
    data = []
    with open(args.data_file, 'r') as f:
        for line in f:
            data.append([num(t) for t in line.split()])
    dev_cutoff = int(.9 * len(data))
    train_xs = np.asarray(data[:dev_cutoff], dtype = dtype)
    dev_xs = np.asarray(data[dev_cutoff:], dtype=dtype) if not args.nodev else None
    return train_xs, dev_xs


def init_model(args):
    if args.cluster_num:
        lambdas = np.zeros(args.cluster_num)
        mus = np.zeros((args.cluster_num, 2))
        if not args.tied:
            sigmas = np.zeros((args.cluster_num, 2, 2))
        else:
            sigmas = np.zeros((2, 2))
        # TODO: randomly initialize clusters (lambdas, mus, and sigmas)
        #np.random.seed(1)
        #mus = np.random.rand(args.cluster_num, 2)
        lambdas[:] = 1.0 / args.cluster_num
        mus = np.random.random([mus.shape[0], mus.shape[1]])
        if not args.tied:
            for i in range(args.cluster_num):
                #sigmas = np.array([np.eye(2)*3 for i in range(args.cluster_num)])
                sigmas[i] = [[4 * np.random.random(1), 0],[0, 4* np.random.random(1)]]
        else:
            sigmas[:] = [[3,0],[0,3]]
            #sigmas = np.eye(2)*3
    else:
        lambdas = []
        mus = []
        sigmas = []
        with open(args.clusters_file, 'r') as f:
            for line in f:
                # each line is a cluster, and looks like this:
                # lambda mu_1 mu_2 sigma_0_0 sigma_0_1 sigma_1_0 sigma_1_1
                lambda_k, mu_k_1, mu_k_2, sigma_k_0_0, sigma_k_0_1, sigma_k_1_0, sigma_k_1_1 = map(float, line.split())
                lambdas.append(lambda_k)
                mus.append([mu_k_1, mu_k_2])
                sigmas.append([[sigma_k_0_0, sigma_k_0_1], [sigma_k_1_0, sigma_k_1_1]])
        lambdas = np.asarray(lambdas)
        mus = np.asarray(mus)
        sigmas = np.asarray(sigmas)
        args.cluster_num = len(lambdas)

    # TODO: do whatever you want to pack the lambdas, mus, and sigmas into the model variable (just a tuple, or a class, etc.)
    # NOTE: if args.tied was provided, sigmas will have a different shape
    model = lambdas, mus, sigmas
    return model


def train_model(model, train_xs, dev_xs, args):
    from scipy.stats import multivariate_normal
    # NOTE: you can use multivariate_normal like this:
    # probability_of_xn_given_mu_and_sigma = multivariate_normal(mean=mu, cov=sigma).pdf(xn)
    # TODO: train the model, respecting args (note that dev_xs is None if args.nodev is True)
    lambdas, mus, sigmas = model

    train_ll = []
    dev_ll = []

    for i in range(args.iterations):
        sig_k_temp = []
        for k in range(args.cluster_num):
            if args.tied:
                sig_k_temp.append(lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas).pdf(train_xs))
            else:
                sig_k_temp.append(lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas[k]).pdf(train_xs))
        sig_k = np.array(sig_k_temp)/np.sum(sig_k_temp, axis = 0)

        sigmas_temp = []
        for k in range(args.cluster_num):
            lambdas[k] = 1 / train_xs.shape[0] * np.sum(sig_k[k])
            mus[k] = np.dot(sig_k[k], train_xs) / np.sum(sig_k[k])
            if args.tied:
                sigmas_temp.append(np.dot(sig_k[k] * (train_xs - mus[k]).T , train_xs - mus[k]))
            else:
                sigmas[k] = np.dot(sig_k[k] * (train_xs - mus[k]).T, train_xs - mus[k]) / np.sum(sig_k[k])
        if args.tied:
            sigmas = np.sum(sigmas_temp, axis = 0) / len(train_xs)
        model =  lambdas, mus, sigmas

        if not args.nodev:
            new_data = np.concatenate((train_xs, dev_xs), axis = 0)
            for i in range(args.iterations):
                sig_k_temp = []
                for k in range(args.cluster_num):
                    if args.tied:
                        sig_k_temp.append(lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas).pdf(new_data))
                    else:
                        sig_k_temp.append(lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas[k]).pdf(new_data))
                sig_k = np.array(sig_k_temp)/np.sum(sig_k_temp, axis = 0)
            sigmas_temp = []
            for k in range(args.cluster_num):
                lambdas[k] = 1 / new_data.shape[0] * np.sum(sig_k[k])
                mus[k] = np.dot(sig_k[k], new_data) / np.sum(sig_k[k])
                if args.tied:
                    sigmas_temp.append(np.dot(sig_k[k] * (new_data - mus[k]).T, new_data - mus[k]))
                else:
                    sigmas[k] = np.dot(sig_k[k] * (new_data - mus[k]).T, new_data - mus[k]) / np.sum(sig_k[k])
            if args.tied:
                sigmas = np.sum(sigmas_temp, axis = 0) / len(new_data)
            model = lambdas, mus, sigmas

            if not args.nodev:
                train_ll.append(average_log_likelihood(model, train_xs, args))
                dev_ll.append(average_log_likelihood(model, dev_xs, args))
        # plot train_ll & dev_ll
        plot_ll(train_ll, dev_ll, args)

    return model

def plot_ll(train_ll, dev_ll, args):
    # plot trian/dev log likelihood
    plt.plot([x for x in range(1, len(train_ll)+1)], train_ll)
    plt.plot([x for x in range(1, len(dev_ll)+1)], dev_ll)
    plt.xlabel('Iteration Times')
    plt.ylabel('Log likelihood')
    plt.legend(labels = ('train', 'dev'))
    plt.title('Log Likelihood \n'+'with iterations: '+str(args.iterations)+' and Cluster Number: '+str(args.cluster_num))
    plt.savefig('log_likelihood_'+'iterations_'+str(args.iterations)+'_cluster_num_'+str(args.cluster_num))
    plt.show()

def average_log_likelihood(model, data, args):
    from math import log
    from scipy.stats import multivariate_normal
    # TODO: implement average LL calculation (log likelihood of the data, divided by the length of the data)
    ll = 0.0
    ll_sum = np.zeros(len(data))
    lambdas, mus, sigmas = model
    for k in range(len(lambdas)):
        if not args.tied:    
            ll_temp = lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas[k]).pdf(data)
        else:
            ll_temp = lambdas[k] * multivariate_normal(mean = mus[k], cov = sigmas).pdf(data)
        ll_sum += (ll_temp)
    ll = sum(np.log(ll_sum))
    ll = ll/(len(data))
    #ll = np.sum(np.log(np.sum(ll_temp, axis = 0))) / len(data)
    return ll


def extract_parameters(model):
    # TODO: extract lambdas, mus, and sigmas from the model and return them (same type and shape as in init_model)
    lambdas, mus, sigmas = model
    return lambdas, mus, sigmas


def main():
    import argparse
    import os
    print('Gaussian')  # Do not change, and do not print anything before this.
    parser = argparse.ArgumentParser(description='Use EM to fit a set of points.')
    init_group = parser.add_mutually_exclusive_group(required=True)
    init_group.add_argument('--cluster_num', type=int, default=4, help='Randomly initialize this many clusters.')
    init_group.add_argument('--clusters_file', type=str, help='Initialize clusters from this file.')
    parser.add_argument('--nodev', action='store_true', help='If provided, no dev data will be used.')
    parser.add_argument('--data_file', type=str, default=os.path.join(DATA_PATH, 'points.dat'), help='Data file.')
    parser.add_argument('--print_params', action='store_true',
                        help='If provided, learned parameters will also be printed.')
    parser.add_argument('--iterations', type=int, default=1, help='Number of EM iterations to perform')
    parser.add_argument('--tied', action='store_true',
                        help='If provided, use a single covariance matrix for all clusters.')
    args = parser.parse_args()
    if args.tied and args.clusters_file:
        print(
            'You don\'t have to (and should not) implement tied covariances when initializing from a file. Don\'t provide --tied and --clusters_file together.')
        exit(1)

    train_xs, dev_xs = parse_data(args)
    model = init_model(args)
    model = train_model(model, train_xs, dev_xs, args)
    ll_train = average_log_likelihood(model, train_xs, args)
    print('Train LL: {}'.format(ll_train))
    if not args.nodev:
        ll_dev = average_log_likelihood(model, dev_xs, args)
        print('Dev LL: {}'.format(ll_dev))
    lambdas, mus, sigmas = extract_parameters(model)
    if args.print_params:
        def intersperse(s):
            return lambda a: s.join(map(str, a))

        print('Lambdas: {}'.format(intersperse(' | ')(np.nditer(lambdas))))
        print('Mus: {}'.format(intersperse(' | ')(map(intersperse(' '), mus))))
        if args.tied:
            print('Sigma: {}'.format(intersperse(' ')(np.nditer(sigmas))))
        else:
            print('Sigmas: {}'.format(intersperse(' | ')(map(intersperse(' '), map(lambda s: np.nditer(s), sigmas)))))


if __name__ == '__main__':
    main()

